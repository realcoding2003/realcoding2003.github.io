---
layout: post
title: "OllamaでローカルLLMを動かしてみた感想 - 結論は用途次第"
date: 2025-02-15 09:00:00 +0900
categories: [Development, AI]
tags: [Ollama, LLM, ローカルAI, Llama, オフライン]
author: "Kevin Park"
lang: ja
excerpt: "ローカルでLLMを動かしたくてOllamaを導入しました。思ったより簡単に動きますが、クラウドAPIと比べると限界は明確です。"
permalink: /ja/:year/:month/:day/:title/
redirect_from:
  - /2025/02/15/ollama-local-llm-review-ja/
---

# OllamaでローカルLLMを動かしてみた

## ローカルでAIを動かしたかった

ChatGPTやClaudeのようなクラウドサービスを使っていると、一つ気になることがあります。自分のコードを外部サーバーに送らなければならないことです。

個人プロジェクトなら構いませんが、クライアントのコードや会社の機密が含まれる作業をする時はちょっと不安です。「データを学習に使用しません」と言われても、やはり。

そこでローカルでLLMを動かしてみることにしました。Ollamaがインストールも簡単でモデル管理も楽だと聞いて選びました。

## セットアップは本当に簡単

```bash
# macOS
brew install ollama

# サービス開始
ollama serve

# モデルダウンロードと実行
ollama run llama3.1
```

これだけです。本当に3行でローカルLLMが動きます。Dockerの設定もGPUドライバも何も必要ありません。

モデルも豊富です。Llama 3.1、CodeLlama、Mistral、Gemmaなどのオープンソースモデルを`ollama pull`一行でダウンロードできます。

## 実際の使用感

**コード作成補助** — CodeLlamaを試しました。簡単な関数作成やコード説明は大丈夫ですが、複雑なリファクタリングやアーキテクチャレベルの提案はGPT-4やClaudeに比べるとかなり劣ります。

**ドキュメント要約** — Llama 3.1でドキュメント要約をさせてみました。英語のドキュメントはまあまあですが、日本語はまだ弱いです。日本語の理解力がクラウドモデルに比べて明らかに劣ります。

**速度** — M4 Mac Miniで動かしましたが、7Bモデルは体感速度が悪くありません。でも70B級モデルはトークン生成速度が遅すぎて実用的ではありません。GPUメモリが鍵です。

## 自分なりの結論

いろいろ試してみて出した結論はこうです。

**ローカルLLMが適しているケース：**
- セキュリティが重要な環境（コード、機密文書）
- オフライン環境で使う必要がある時
- 単純な繰り返し作業（フォーマット変換、簡単な分類など）
- APIコストを節約したい時

**クラウドLLMの方が良いケース：**
- 複雑な推論が必要な時
- 日本語処理
- 最新の知識が必要な時
- 長いコンテキストを扱う時

結局「用途次第」というありきたりな結論ですが、実際に使ってみるとこの「用途」の境界が明確になりました。ローカルLLMはクラウドを代替するのではなく、補完する関係なのです。

業務用にはまだClaudeやGPTをメインで使いつつ、セキュリティが必要な作業でのみOllamaを使うのが現実的だと思います。オープンソースモデルが急速に発展しているので、今後は変わるかもしれませんが。
