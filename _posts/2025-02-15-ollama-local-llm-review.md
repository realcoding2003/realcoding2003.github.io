---
layout: post
title: "Ollama로 로컬 LLM 돌려본 후기 - 결론은 용도에 따라 다르다"
date: 2025-02-15 09:00:00 +0900
categories: [Development, AI]
tags: [Ollama, LLM, 로컬AI, llama, 오프라인]
author: "Kevin Park"
lang: ko
excerpt: "로컬에서 LLM을 돌려보고 싶어서 Ollama를 설치했다. 생각보다 쉽게 돌아가긴 하는데, 클라우드 API와 비교하면 한계가 명확하다."
---

# Ollama로 로컬 LLM 돌려본 후기

## 로컬에서 AI를 돌리고 싶었다

ChatGPT, Claude 같은 클라우드 서비스를 쓰다 보면 한 가지 꺼림칙한 게 있다. 내 코드를 외부 서버로 보내야 한다는 거다.

개인 프로젝트야 상관없는데, 클라이언트 코드나 회사 기밀이 포함된 작업을 할 때는 좀 찝찝하다. "데이터를 학습에 사용하지 않습니다"라고 하지만 그래도.

그래서 로컬에서 LLM을 돌려보기로 했다. Ollama가 설치도 간단하고 모델 관리도 편하다고 해서 선택했다.

## 설치는 진짜 간단하다

```bash
# macOS
brew install ollama

# 서비스 시작
ollama serve

# 모델 다운로드 및 실행
ollama run llama3.1
```

이게 끝이다. 정말 3줄이면 로컬에서 LLM이 돌아간다. Docker 설정이니 GPU 드라이버니 복잡한 거 하나도 없다.

모델도 다양하다. Llama 3.1, CodeLlama, Mistral, Gemma 등 오픈소스 모델들을 `ollama pull` 한 줄로 받을 수 있다.

## 실사용 느낌

**코드 작성 보조** — CodeLlama를 돌려봤다. 간단한 함수 작성이나 코드 설명은 괜찮은데, 복잡한 리팩토링이나 아키텍처 수준의 제안은 GPT-4나 Claude에 비하면 많이 부족하다.

**문서 요약** — Llama 3.1로 문서 요약을 시켜봤다. 영문 문서는 꽤 괜찮은데, 한국어는 아직 약하다. 한국어 이해력이 클라우드 모델 대비 확연히 떨어진다.

**속도** — M4 맥미니에서 돌렸는데, 7B 모델은 체감 속도가 나쁘지 않다. 근데 70B급 모델은 토큰 생성 속도가 너무 느려서 실용적이지 않다. GPU 메모리가 관건이다.

## 혼자 내린 결론

이것저것 써보고 내린 결론은 이렇다.

**로컬 LLM이 적합한 경우:**
- 보안이 중요한 환경 (코드, 기밀 문서)
- 오프라인 환경에서 써야 할 때
- 단순 반복 작업 (포맷 변환, 간단한 분류 등)
- API 비용을 아끼고 싶을 때

**클라우드 LLM이 나은 경우:**
- 복잡한 추론이 필요할 때
- 한국어 처리
- 최신 지식이 필요할 때
- 긴 컨텍스트를 다룰 때

결국 "용도에 따라 다르다"라는 뻔한 결론이긴 한데, 실제로 써보니까 이 "용도"의 경계가 명확해졌다. 로컬 LLM이 클라우드를 대체하는 게 아니라 보완하는 관계인 거다.

업무용으로는 아직 Claude나 GPT를 주력으로 쓰되, 보안이 필요한 작업에서만 Ollama를 쓰는 게 현실적인 것 같다. 오픈소스 모델이 빠르게 발전하고 있으니까 앞으로는 달라질 수도 있지만.
